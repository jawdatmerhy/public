# importing essential libraries
# Install Keras Tuner
#!pip install keras-tuner

from re import X
import pandas as pd
import missingno as msno
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.stats.mstats import winsorize
import warnings
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from sklearn.utils import class_weight



import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import plotly.graph_objects as go
from pandas.plotting import parallel_coordinates

import warnings
warnings.filterwarnings("ignore")

print("reading csv file")
# reading csv file
df = pd.read_csv('/content/sample_data/loan.csv')


print("\n>>>>>>>>>>>>>> Printing first five rows of dataset")
# printing first five rows of dataset
print(df.head(5))

print("\n>>>>>>>>>>>>>> Printing last five rows of datset")
# Printing last five rows of datset
print(df.tail(5))

print("\n>>>>>>>>>>>>>> Obtaining the dimensions of dataset")
# Obtaining the dimensions of dataset
print(df.shape)

print("\n>>>>>>>>>>>>>> Display the list the columns")
print(df.columns)

print("\n>>>>>>>>>>>>>> Statistical summary of dataset")
# Statistical summary of dataset
df.info()

print("\n>>>>>>>>>>>>>> Calculate summary statistics for numerical columns ")
#Calculate summary statistics for numerical columns using describe()
print(df.describe())

print("\n>>>>>>>>>>>>>> Calculate the total of null values")
# Calculate the total of null values
print(df.isnull().sum().sum())

print("\n>>>>>>>>>>>>>> Visulize the null values using heatmp")
# Visulize the null values using heatmp
df.isnull().sum()
sns.heatmap(df.isnull(), cbar=True, cmap='inferno')
plt.show()

print("\n>>>>>>>>>>>>>> Find the duplicated rows")
#Find the duplicated rows
print(df.duplicated().sum())

print("\n>>>>>>>>>>>>>> To remove duplicated lines if they exist")
print("Calculate the percentage of duplicated rows")
# Calculate the percentage of duplicated rows
percent_duplication = df.duplicated().sum() / df.shape[0]
print(f"Percentage of duplicated rows: {percent_duplication * 100:.2f}%")

print("\n>>>>>>>>>>>>>> Check if there are any duplicated rows and if the percentage is less than 5%")
# Check if there are any duplicated rows and if the percentage is less than 5%
if 0 < percent_duplication < 0.05:
    print("\n>>>>>>>>>>>>>> Drop duplicated rows")
    # Drop duplicated rows
    df = df.drop_duplicates()
else:
    print("There are no duplicated rows or the percentage is 5% or higher")

# Optional: print the percentage of duplication
print(f"Percentage of duplicated rows: {percent_duplication * 100:.2f}%")

print("\n>>>>>>>>>>>>>> Dealing with Categorical values")
 # Dealing with Categorical values

# Gender Column
df['Gender'] =  df['Gender'].map({'Male':0,'Female':1})

# Married column
df['Married'] = df['Married'].map({'No':0,'Yes':1})

# Loan_Status column
df['Loan_Status'] =df['Loan_Status'].map({'N':0,'Y':1})

#Missing Value
# Gender column
df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])
# Married column
df['Married'] = df['Married'].fillna(df['Married'].mode()[0])
# Dependents Column
df['Dependents'] = df['Dependents'].fillna(df['Dependents'].mode()[0])
# Self_Employed Column
df['Self_Employed'].fillna('No',inplace=True)
# Credit_History Column
df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mode()[0])
# LoanAmount Column
df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].median())
# Loan_Amount Column
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0])


# Counting the accurance of each value in Gender column
df['Gender'].value_counts()


plt.figure(figsize=(15, 6))
sns.countplot(x='Gender', data=df, palette='hls')
plt.show()

# Counting the accurance of each value in Dependent column
df['Dependents'].value_counts()

plt.figure(figsize=(15,6))
#sns.countplot('Dependents', data = df, palette='hls')
plt.show()

# comparing loan status with gender column
plt.figure(figsize=(15,6))
sns.countplot(x = 'Gender',hue ='Loan_Status', data=df , palette='hls')
plt.show()

#More males are on loan than females. Also, those that are on loan are more than otherwise
# comparing loan status with married column
plt.figure(figsize = (15,6))
sns.countplot( x='Married', hue ='Loan_Status', data = df)

#Married people collect more loan than unmarried
#Counting the occurence of each value with Loan_amount_term column
df['Loan_Amount_Term'].value_counts()
plt.figure(figsize=(15,6))
#sns.countplot('Loan_Amount_Term', data = df, palette='hls')
plt.xticks(rotation = 90)
plt.show()


print("\n>>>>>>>>>>>>>> box plot dataset")
plt.figure(figsize=(15,10))
sns.boxplot(data=df)
plt.show()

print("\n>>>>>>>>>>>>>> Q1, Q3, and IQR for dataset")
dfOutliers = df[['ApplicantIncome', 'CoapplicantIncome']]
# Calculate Q1, Q3, and IQR
Q1 = dfOutliers.quantile(0.25)
Q3 = dfOutliers.quantile(0.75)
#Q1 = np.percentile(dfAge, 25, interpolation = 'midpoint')
#Q3 = np.percentile(dfAge, 75, interpolation = 'midpoint')

IQR = Q3 - Q1
# Define lower and upper bounds
lowerBound = Q1 - 1.5 * IQR
upperBound = Q3 + 1.5 * IQR
print("Outliers lowerBound: \n", lowerBound)
print("Outliers upperBound: \n", upperBound)
# Identify potential outliers
potentialOutliers = dfOutliers[(dfOutliers < lowerBound) | (dfOutliers > upperBound)]
potentialOutliersRowCount = potentialOutliers.notnull().sum()
print("\nSum of each outliers: \n", potentialOutliersRowCount)

print("\n>>>>>>>>>>>>>> Calculate the number of unique rows containing outliers")
# Get unique rows containing outliers
uniqueOutlierRows = potentialOutliers.any(axis=1)
dfUniqueOutliers = df[uniqueOutlierRows]
# Count the total number of unique rows with outliers
totalUniqueOutliers = dfUniqueOutliers.shape[0]
print("\nTotal number of unique rows with outliers: \n", totalUniqueOutliers)


print("\n>>>>>>>>>>>>>> Calculate the outliers percentage")
# Calculate the outliers percentag
percentOutliers = totalUniqueOutliers / df.shape[0]
print(f"Percentage of duplicated rows: {percentOutliers * 100:.2f}%")

print("\n>>>>>>>>>>>>>> Check if there are any duplicated rows and if the percentage is less than 5%")
# Check if there are any duplicated rows and if the percentage is less than 5%
if 0 < percentOutliers < 0.05:
    print("\n>>>>>>>>>>>>>> Remove outliers")
else:
    print("We can not remove all outliers, percengtage is higer than 5%")

dfWinsorized = df.copy()

 # Winsorize the column 'ApplicantIncome', ApplicantIncome is the column with potential outliers,  2/1030
dfWinsorized['ApplicantIncome_winsorized'] = winsorize(dfWinsorized['ApplicantIncome'], limits=(0.012, 0.012))
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['ApplicantIncome'], kde=True)
plt.title('Before Winsorization of ApplicantIncome')
plt.subplot(1, 2, 2)
sns.histplot(dfWinsorized['ApplicantIncome_winsorized'], kde=True)
plt.title('After Winsorization ApplicantIncome')
plt.show()
#Replace orginal value
df['ApplicantIncome'] = dfWinsorized['ApplicantIncome_winsorized']


count = df[df['ApplicantIncome'] < 20000].shape[0]
percentilleApplicantIncome = (df.shape[0] - count) / df.shape[0] / 100
print('ApplicantIncome count < 10000' , count)
print('ApplicantIncome perc < 10000' , percentilleApplicantIncome)

count = df[df['CoapplicantIncome'] < 10000].shape[0]
percentilleCoapplicantIncome= (df.shape[0] - count) / df.shape[0]
print('CoapplicantIncome perc < 10000' , percentilleApplicantIncome)
print('CoapplicantIncome count < 10000' , count)

 # Winsorize the column 'CoapplicantIncome', CoapplicantIncome is the column with potential outliers
dfWinsorized['CoapplicantIncome_winsorized'] = winsorize(dfWinsorized['CoapplicantIncome'], limits=(0.009, 0.009))
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['CoapplicantIncome'], kde=True)
plt.title('Before Winsorization of CoapplicantIncome')
plt.subplot(1, 2, 2)
sns.histplot(dfWinsorized['CoapplicantIncome_winsorized'], kde=True)
plt.title('After Winsorization CoapplicantIncome')
plt.show()
#Replace orginal value
df['CoapplicantIncome'] = dfWinsorized['CoapplicantIncome_winsorized']


print("\n>>>>>>>>>>>>>> Q1, Q3, and IQR for dataset")
dfOutliers = df[['ApplicantIncome', 'CoapplicantIncome']]
# Calculate Q1, Q3, and IQR
Q1 = dfOutliers.quantile(0.25)
Q3 = dfOutliers.quantile(0.75)
#Q1 = np.percentile(dfAge, 25, interpolation = 'midpoint')
#Q3 = np.percentile(dfAge, 75, interpolation = 'midpoint')

IQR = Q3 - Q1
# Define lower and upper bounds
lowerBound = Q1 - 1.5 * IQR
upperBound = Q3 + 1.5 * IQR
print("Outliers --lowerBound: \n", lowerBound)
print("Outliers --upperBound: \n", upperBound)
# Identify potential outliers
potentialOutliers = dfOutliers[(dfOutliers < lowerBound) | (dfOutliers > upperBound)]
potentialOutliersRowCount = potentialOutliers.notnull().sum()
print("\nSum of each outliers: \n", potentialOutliersRowCount)

print("\n>>>>>>>>>>>>>> Calculate the number of unique rows containing outliers")
# Get unique rows containing outliers
uniqueOutlierRows = potentialOutliers.any(axis=1)
dfUniqueOutliers = df[uniqueOutlierRows]
# Count the total number of unique rows with outliers
totalUniqueOutliers = dfUniqueOutliers.shape[0]
print("\nTotal number of unique rows with outliers---: \n", totalUniqueOutliers)


print("\n>>>>>>>>>>>>>> Calculate the outliers percentage")
# Calculate the outliers percentag
percentOutliers = totalUniqueOutliers / df.shape[0]
print(f"Percentage of duplicated rows: {percentOutliers * 100:.2f}%")

print("\n>>>>>>>>>>>>>> Check if there are any duplicated rows and if the percentage is less than 5%")
# Check if there are any duplicated rows and if the percentage is less than 5%
if 0 < percentOutliers < 0.05:
    print("\n>>>>>>>>>>>>>> Remove outliers")
else:
    print("We can not remove all outliers, percengtage is higer than 5%")


# plotting boxplot
plt.figure(figsize=(15,6))
sns.boxplot(x='Loan_Status',y = 'ApplicantIncome', data=df)
plt.show()

plt.figure(figsize=(15,6))
sns.boxplot(x='Loan_Status',y = 'CoapplicantIncome', data=df)
plt.show()




# Showing correlation through heatmap
plt.figure(figsize=(7,7))
#sns.heatmap(df.corr())

plt.figure(figsize=(15,6))
sns.countplot(x = 'Property_Area', hue='Loan_Status', data = df)

#Semiurban obtain more loan, folowed by Urban and then rural.
plt.figure(figsize=(15,6))
sns.countplot(x = 'Credit_History', hue='Loan_Status', data = df)

#According to the credit history, greater number of people pay back their loans.
plt.figure(figsize=(15,6))
sns.countplot(x = 'Loan_Amount_Term', hue='Loan_Status', data = df)

#An extremely high number of them go for a 360 cyclic loan term. That’s pay back within a year

plt.figure(figsize=(15,6))
sns.countplot( x = 'Self_Employed', hue = 'Loan_Status', data = df)

X = df[['Gender', 'Married', 'ApplicantIncome', 'LoanAmount',
'Credit_History']]
y = df.Loan_Status

print('Data Splitting')
# Data Splitting

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2,
random_state = 10)

#Random forest algorithm
model = RandomForestClassifier(max_depth=4, random_state = 10)
model.fit(X_train, y_train)

RandomForestClassifier(max_depth=4, random_state=10)
pred_cv = model.predict(X_test)
accuracy_score(y_test,pred_cv)

pred_train = model.predict(X_train)
print("Accuracy Random forest algorithm: ",accuracy_score(y_train,pred_train))

#Logistic Regression
model = LogisticRegression()
model.fit(X_train, y_train)
LogisticRegression()
from sklearn.metrics import accuracy_score
y_pred= model.predict(X_train)
print("Accuracy Logistic Regression : ",accuracy_score(y_pred,y_train))

#Linear svm
model = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)
model.fit(X_train, y_train)
SGDClassifier(alpha=0.001, max_iter=15, random_state=5, tol=None)
from sklearn.metrics import accuracy_score
y_pred = model.predict(X_train)
print("Accuracy Linear svm: ",accuracy_score(y_pred,y_train))



print("Deep learning");

# Build the model
model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy*100:.2f}%')



# Calculate class weights to handle imbalanced data
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))


# Enhanced deep learning model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

optimizer = Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Training the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, 
                    class_weight=class_weights_dict, callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
y_pred = model.predict(X_test).flatten()
roc_auc = roc_auc_score(y_test, y_pred)

print(f'Deep Learning Model Accuracy: {accuracy * 100:.2f}%')
print(f'Deep Learning Model AUC-ROC: {roc_auc:.2f}')



from kerastuner.tuners import RandomSearch

def build_model(hp):
    model = Sequential()
    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=32), activation='relu', input_dim=X_train.shape[1]))
    model.add(Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1)))
    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=512, step=32), activation='relu'))
    model.add(Dropout(hp.Float('dropout_2', 0.2, 0.5, step=0.1)))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='log')),
                  loss='binary_crossentropy', metrics=['accuracy'])
    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=3,
    directory='my_dir',
    project_name='loan_approval'
)

tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])

# Get the optimal hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"Best hyperparameters: {best_hps.values}")

# Train the best model
best_model = tuner.hypermodel.build(best_hps)
history = best_model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])

# Evaluate the best model
loss, accuracy = best_model.evaluate(X_test, y_test)
y_pred = best_model.predict(X_test).flatten()
roc_auc = roc_auc_score(y_test, y_pred)

print(f'Best Deep Learning Model Accuracy: {accuracy * 100:.2f}%')
print(f'Best Deep Learning Model AUC-ROC: {roc_auc:.2f}')


