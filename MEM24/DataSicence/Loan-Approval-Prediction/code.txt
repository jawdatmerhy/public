# importing essential libraries
# Install Keras Tuner
#!pip install keras-tuner

from re import X
import pandas as pd

import missingno as msno
from IPython.display import display, HTML
from io import StringIO
from termcolor import colored # For colored text printing
from tabulate import tabulate
from scipy.stats import chi2_contingency
from prettytable import PrettyTable
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.stats.mstats import winsorize
import warnings
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from xgboost import XGBClassifier, XGBRegressor
from lightgbm import LGBMClassifier, LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.neural_network import MLPClassifier
from sklearn.utils import class_weight
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score
from sklearn.model_selection import learning_curve
from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report
import plotly.express as px


import plotly.graph_objects as go
from pandas.plotting import parallel_coordinates

import warnings
warnings.filterwarnings("ignore")

def display_colored_message(message, color='black', background='orange'):
    display(HTML(f'<span style="color: {color}; background-color: {background};">{message}</span>'))

def display_colored_sub_message(message):
    print("\033[1;33;44m" + message + "\033[0m")


display_colored_message("THE REQUIRED LIBRARIES WERE SUCCESSFULLY IMPORTED...")

####################################################
try:
    # Read the CSV file and save it in "loan_data" variable
    # reading csv file
    df = pd.read_csv('/content/sample_data/loan.csv')

    # Print a success message
    display_colored_message("THE DATASET LOADED SUCCESSFULLY...")

except FileNotFoundError:
    # Handle file not found error
    print(colored("ERROR: File not found!", "red", attrs=['reverse']))

except Exception as e:
    # Handle other exceptions
    print(colored(f"ERROR: {e}", "red", attrs=['reverse']))

####################################################
display_colored_message("PRINTING THE FIRST FIVE ROWS OF THE DATASET...")
# printing first five rows of dataset

print(colored(tabulate(df.head(5), headers='keys', tablefmt='fancy_grid'), 'cyan'))

####################################################
display_colored_message("PRINTING THE LAST FIVE ROWS OF THE DATASET...")
# Printing last five rows of datset
print(colored(tabulate(df.tail(5), headers='keys', tablefmt='fancy_grid'), 'cyan'))

####################################################
display_colored_message("OBTAINING THE DIMENSIONS OF THE DATASET...")
# Obtaining the dimensions of dataset
print("The shape =", df.shape)

# Dataset dimensions and statistics
num_rows, num_cols = df.shape
num_features = num_cols - 1
num_data = num_rows * num_cols

# Print the information about the dataset
print(f"Number of Rows: {num_rows}")
print(f"Number of Columns: {num_cols}")
print(f"Number of Features: {num_features}")
print(f"Number of All Data: {num_data}")

####################################################
display_colored_message("DISPLAY THE LIST COLUMNS...")
print(df.columns)

####################################################
display_colored_message("RENAME COLUMNS...")

#Rename columuns Self_Employed, Credit_History, Loan_Amount_Term
def rename_columns(df):
    # Rename columns in a single call
    df = df.rename(columns={
        'Self_Employed': 'SelfEmployed',
        'Credit_History': 'CreditHistory',
        'Loan_Amount_Term': 'LoanAmountTerm',
        'Property_Area': 'PropertyArea'
    })
    return df

#RENAME COLUMNS
df = rename_columns(df)

print(df.columns)
####################################################
display_colored_message("STATISTICAL SUMMARY OF THE DATASET...")
# Statistical summary of dataset
#df.info()

# Function to extract info into a DataFrame
def extract_info(df):
    buffer = StringIO()
    df.info(buf=buffer)
    s = buffer.getvalue()

    # Parse the buffer content to extract column info
    lines = s.split('\n')
    col_lines = lines[5:-3]  # Data lines

    memory_line = lines[-2]  # Memory usage line

    info_dict = {
        "Column": [],
        "Non-Null Count": [],
        "Dtype": []
    }

    for line in col_lines:
        parts = line.split()
        info_dict["Column"].append(parts[1])
        info_dict["Non-Null Count"].append(int(parts[2].replace(',', '')))
        info_dict["Dtype"].append(parts[-1])

    info_df = pd.DataFrame(info_dict)
    return info_df

# Extract info and create a DataFrame
info_df = extract_info(df)

# Convert Non-Null Count to a proportion for better gradient visualization
info_df['Non-Null Proportion'] = info_df['Non-Null Count'] / len(df)

# Style the DataFrame
styled_info_df = info_df.style.background_gradient(cmap='viridis', subset=['Non-Null Proportion'])

# Display the styled DataFrame
display(styled_info_df)

####################################################
display_colored_message("SUMMARY STATISTICTS FOR NUMERICAL COLUMNS...")
#Calculate summary statistics for numerical columns using describe()
#print(df.describe())

# Describe and style
styled_df = df.describe().T.style.background_gradient(cmap='viridis')

# Use display to explicitly show the styled DataFrame
display(styled_df)

####################################################
display_colored_message("TOTAL OF NULL VALUES...")
# Calculate the total of null values
print(df.isnull().sum().sum())

display_colored_message("VISULIZE THE NULL VALUES USING HEATMAP...")
# Visulize the null values using heatmp
df.isnull().sum()
sns.heatmap(df.isnull(), cbar=True, cmap='inferno')
plt.show()

####################################################
display_colored_message("FIND THE DUPLICATED ROWS...")
#Find the duplicated rows
print(df.duplicated().sum())

display_colored_sub_message("\n>>>>>>>>>>>>>> To remove duplicated lines if they exist")
print("Calculate the percentage of duplicated rows")
# Calculate the percentage of duplicated rows
percent_duplication = df.duplicated().sum() / df.shape[0]
print(f"Percentage of duplicated rows: {percent_duplication * 100:.2f}%")


display_colored_sub_message("\n>>>>>>>>>>>>>> Check if there are any duplicated rows and if the percentage is less than 5%")
# Check if there are any duplicated rows and if the percentage is less than 5%
if 0 < percent_duplication < 0.05:
    display_colored_sub_message("\n>>>>>>>>>>>>>> Drop duplicated rows")
    # Drop duplicated rows
    df = df.drop_duplicates()
else:
    print("There are no duplicated rows or the percentage is 5% or higher")

# Optional: print the percentage of duplication
print(f"Percentage of duplicated rows: {percent_duplication * 100:.2f}%")

fig = go.Figure(go.Indicator(
    mode = "gauge+number",
    gauge = {
       'axis': {'range': [None, 100]}},
    value = percent_duplication,
    title = {'text': "Duplication %"},
    domain = {'x': [0, 1], 'y': [0, 1]}
))
fig.show()


####################################################
display_colored_message("FILL MISSING VALUES...")

null_applicationDF = pd.DataFrame((df.isnull().sum())*100/df.shape[0]).reset_index()
null_applicationDF.columns = ['Column Name', 'Null Values Percentage']
fig = plt.figure(figsize=(18,6))
ax = sns.pointplot(x="Column Name",y="Null Values Percentage",data=null_applicationDF,color='blue')
plt.xticks(rotation =90,fontsize =7)
ax.axhline(15, ls='--',color='red')
plt.title("Percentage of Missing values in application data")
plt.ylabel("Null Values PERCENTAGE")
plt.xlabel("COLUMNS")
plt.show()

#Missing Value
def data_prep_fill_mising_values(data):
  # Gender column
  data['Gender'] = data['Gender'].fillna(data['Gender'].mode()[0])
  # Married column
  data['Married'] = data['Married'].fillna(data['Married'].mode()[0])
  # Dependents Column
  data['Dependents'] = data['Dependents'].fillna(data['Dependents'].mode()[0])
  # Self_Employed Column
  #data['SelfEmployed'].fillna('No',inplace=True)
  data['SelfEmployed'] = data['SelfEmployed'].fillna(data['SelfEmployed'].mode()[0])
  # Credit_History Column
  data['CreditHistory'] = data['CreditHistory'].fillna(data['CreditHistory'].mode()[0])
  # LoanAmountTerm Column
  data['LoanAmountTerm'] = data['LoanAmountTerm'].fillna(data['LoanAmountTerm'].mode()[0])
  # LoanAmount Column
  data['LoanAmount'] = data['LoanAmount'].fillna(data['LoanAmount'].median())

data_prep_fill_mising_values(df)

print('Total missing values After fillna : ', df.isnull().sum().sum())

####################################################
display_colored_message("\nDEALING WITH CATEGORICAL VALUES...")
# Dealing with Categorical values

# Gender Column
#df['Gender'] =  df['Gender'].map({'Male':0,'Female':1})
le = LabelEncoder()
#df["Gender"] = le.fit_transform(df["Gender"])
#df["Married"] = le.fit_transform(df["Married"])
df["Loan_Status"] = le.fit_transform(df["Loan_Status"])

print(colored(tabulate(df.head(2), headers='keys', tablefmt='fancy_grid'), 'cyan'))

##############################################
display_colored_message("\nchi2 CHECK...")

# Filter columns with 'object' dtype
categorical_columns = df.select_dtypes(include=['object']).columns
display_colored_sub_message("\nNull Hypothesis: there is no relation between categorical variables and Loan_Status")

chi2_check = []
for i in categorical_columns:
    if chi2_contingency(pd.crosstab(df['Loan_Status'], df[i]))[1] < 0.05:
        chi2_check.append('Reject Null Hypothesis')
    else:
        chi2_check.append('Fail to Reject Null Hypothesis')


res = pd.DataFrame({'Column': categorical_columns, 'Hypothesis': chi2_check})

# Define a function to color the cells
def color_hypothesis(val):
    color = 'green' if val == 'Reject Null Hypothesis' else 'red'
    return f'color: {color}'

# Apply the coloring function to the Hypothesis column
styled_res = res.style.applymap(color_hypothesis, subset=['Hypothesis'])

# Display the styled DataFrame
styled_res

####################################################
display_colored_message("DROP COLUMNS...")
# Drop columns
columns_to_drop = ['Loan_ID']
df.drop(columns_to_drop, axis=1, inplace=True)


#TO BE USED FOR REMOVE ALL DROPPED COLUMNS IN TESTING
ALL_DROPPED_COLUMNS = columns_to_drop;

####################################################
display_colored_message("DISPLAY CORRELATION...")
dfCopy = df.copy()
# Filter columns with 'object' dtype
object_columns = dfCopy.select_dtypes(include=['object']).columns

# Initialize LabelEncoder
le = LabelEncoder()

# Apply LabelEncoder to 'object' columns
for col in object_columns:
    dfCopy[col] = le.fit_transform(dfCopy[col])

plt.figure(figsize=(10,8))
sns.heatmap(dfCopy.corr(),annot=True,cmap="viridis")
plt.show()


###################################################
display_colored_message("BOX PLOT DATASET...")
plt.figure(figsize=(15,10))
sns.boxplot(data=df)
plt.show()

####################################################
display_colored_message("DISPLAY OUTLIERS...")
# Set the figure size
plt.figure(figsize=(15, 10))

# Only variables that have outliers
outliersColumns = df.get(["ApplicantIncome", "CoapplicantIncome", "LoanAmount", "LoanAmountTerm"])

# Add outliers to the plot
sns.stripplot(data=outliersColumns, color="red", jitter=0.3, size=5)

# Set the axis labels and title
plt.title("Outliers")

# Show the plot
plt.show()

####################################################
display_colored_message("HANDLE OUTLIERS...")

def calculate_outliers(df, columns):
    Q1 = df[columns].quantile(0.25)
    Q3 = df[columns].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    potential_outliers = df[columns][(df[columns] < lower_bound) | (df[columns] > upper_bound)]
    return lower_bound, upper_bound, potential_outliers

def plot_pie_chart(percent_outliers):
    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect="equal"))
    sizes = [percent_outliers * 100, 100 - (percent_outliers * 100)]
    colors = ['#66c2a5', 'Yellow']
    wedges, _ = ax.pie(sizes, colors=colors, startangle=90, wedgeprops=dict(width=0.3))
    plt.text(0, 0, f'{percent_outliers * 100:.2f}%', ha='center', va='center', fontsize=20, weight='bold')
    plt.title('Percentage of outliers')
    plt.show()

columns_to_check = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
lower_bound, upper_bound, potential_outliers = calculate_outliers(df, columns_to_check)

print("Outliers lowerBound: \n", lower_bound)
print("Outliers upperBound: \n", upper_bound)

potential_outliers_row_count = potential_outliers.notnull().sum()
display_colored_message(f"\nSum of each outliers: {potential_outliers_row_count}", "blue")

unique_outlier_rows = potential_outliers.any(axis=1)
df_unique_outliers = df[unique_outlier_rows]
total_unique_outliers = df_unique_outliers.shape[0]
display_colored_message(f"\nTotal number of unique rows with outliers: {total_unique_outliers}", "blue")

percent_outliers = total_unique_outliers / df.shape[0]
display_colored_message(f"Percentage of outliers before treatment : {percent_outliers * 100:.2f}%", "blue")

plot_pie_chart(percent_outliers)

if 0 < percent_outliers < 0.05:
    display_colored_message("\nRemoving outliers", "blue")
else:
    print("Outliers percentage too high > 5%")

####################################################
display_colored_message("REMOVE OUTLIERS...")

def winsorize_column(df, column, lower_limit, upper_limit):
    count = df[df[column] < upper_limit].shape[0]
    percentile = (df.shape[0] - count) / df.shape[0]
    display_colored_message(f'{column} [count < {upper_limit}] : {count}', "blue")
    display_colored_message(f'{column} [perc < {upper_limit}] : {percentile}', "blue")
    winsorized_column = f'{column}_winsorized'
    df[winsorized_column] = winsorize(df[column], limits=(lower_limit, upper_limit))
    plot_histograms(df, column, winsorized_column)
    df[column] = df[winsorized_column]

def plot_histograms(df, column, winsorized_column):
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sns.histplot(df[column], kde=True)
    plt.title(f'Before Winsorization of {column}')
    plt.subplot(1, 2, 2)
    sns.histplot(df[winsorized_column], kde=True)
    plt.title(f'After Winsorization {column}')
    plt.show()

def plot_stripplot(df, columns):
    plt.figure(figsize=(12, 6))
    sns.stripplot(data=df[columns], color="red", jitter=0.3, size=5)
    plt.title("Outliers")
    plt.show()

def plot_boxplots(df, columns):
    for column in columns:
        plt.figure(figsize=(8, 3))
        sns.boxplot(x='Loan_Status', y=column, data=df)
        plt.title(f'Boxplot of {column}')
        plt.show()


df_winsorized = df.copy()

# Winsorize the column 'ApplicantIncome', ApplicantIncome is the column with potential outliers,  ~ 50/614 = 0.0815
winsorize_column(df_winsorized, 'ApplicantIncome', 0.0001, 0.084)

# Winsorize the column 'CoapplicantIncome', CoapplicantIncome is the column with potential outliers ~ 18/614 = 0.0295
winsorize_column(df_winsorized, 'CoapplicantIncome', 0.0001, 0.031)

# Winsorize the column 'LoanAmount', LoanAmount is the column with potential outliers ~ 41/614 = 0.067
winsorize_column(df_winsorized, 'LoanAmount', 0.0001, 0.085)

df[columns_to_check] = df_winsorized[columns_to_check]

lower_bound, upper_bound, potential_outliers = calculate_outliers(df, columns_to_check)

print("Outliers --lowerBound:\n", lower_bound)
print("Outliers --upperBound:\n", upper_bound)
potential_outliers_row_count = potential_outliers.notnull().sum()
print("\nSum of each outliers:\n", potential_outliers_row_count)

unique_outlier_rows = potential_outliers.any(axis=1)
df_unique_outliers = df[unique_outlier_rows]
total_unique_outliers = df_unique_outliers.shape[0]
print("\nTotal number of unique rows with outliers:\n", total_unique_outliers)

percent_outliers = total_unique_outliers / df.shape[0]
print(f"Percentage of outliers: {percent_outliers * 100:.2f}%")
plot_pie_chart(percent_outliers)

plot_stripplot(df, columns_to_check)

plot_boxplots(df, columns_to_check)

####################################################
display_colored_message("DATA VISULAZATION...")

####################################################
# Counting the accurance of each value in Gender column
df['Gender'].value_counts()

plt.figure(figsize=(12, 4))
sns.countplot(x='Gender', data=df, palette='hls')
plt.show()

# comparing loan status with gender column
plt.figure(figsize=(12,4))
sns.countplot(x = 'Gender',hue ='Loan_Status', data=df , palette='hls')
plt.show()

#Married people collect more loan than unmarried
#Counting the occurence of each value with Loan_amount_term column
df['LoanAmountTerm'].value_counts()
plt.figure(figsize=(15,6))
#sns.countplot('Loan_Amount_Term', data = df, palette='hls')
#plt.xticks(rotation = 90)
#plt.show()

# Creating cross-tabulations
crosstab1 = pd.crosstab(df.Gender, df.Married)
crosstab2 = pd.crosstab(df.Gender, df.Loan_Status)
crosstab3 = pd.crosstab(df.Gender, df.CreditHistory)

# Plotting the cross-tabulations in subplots
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Gender vs Married
crosstab1.plot(kind="bar", stacked=True, ax=axes[0], color=['#f64f59', '#12c2e9'])
axes[0].set_title('Gender vs Married')
axes[0].set_xlabel('Gender')
axes[0].set_ylabel('Frequency')
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)

# Gender vs Loan_Status
crosstab2.plot(kind="bar", stacked=True, ax=axes[1], color=['#FF5733', '#FFC300'])
axes[1].set_title('Gender vs Loan_Status')
axes[1].set_xlabel('Gender')
axes[1].set_ylabel('Frequency')
axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)

# Gender vs LoanAmount
# For CreditHistory, we will use a different kind of plot since it's continuous data.
crosstab3.plot(kind="bar", stacked=True, ax=axes[2], color=['#C70039', '#FF5733'])
axes[2].set_title('Gender vs CreditHistory')
axes[2].set_xlabel('Gender')
axes[2].set_ylabel('Frequency')
axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=0)

plt.tight_layout()
plt.show()


####################################################
display_colored_message("3D SCATTER PLOT OF LOAN DATA...")

plt.figure(figsize=(15, 10))
fig = px.scatter_3d(df,
                    x='ApplicantIncome',
                    y='CoapplicantIncome',
                    z='LoanAmount',
                    color='Loan_Status',
                    size='CreditHistory',
                    size_max=18,
                    symbol='Gender',
                    opacity=0.7,
                    title='3D Scatter Plot of Loan Data',
                    labels={
                        'ApplicantIncome': 'Applicant Income',
                        'CoapplicantIncome': 'Coapplicant Income',
                        'LoanAmount': 'Loan Amount',
                        'Loan_Status': 'Loan Status',
                        'CreditHistory': 'Credit History',
                        'Gender': 'Gender'
                    })

fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),
                  scene=dict(
                      xaxis_title='Applicant Income',
                      yaxis_title='Coapplicant Income',
                      zaxis_title='Loan Amount',
                  ))

fig.show()


display_colored_message("3D SCATTER PLOT OF LOAN DATA BY LOAN STATUS AND GENDER...")

plt.figure(figsize=(15, 10))
fig = px.scatter_3d(df,
                    x='LoanAmount',
                    y='ApplicantIncome',
                    z='LoanAmountTerm',
                    color='Loan_Status',
                    size='CreditHistory',
                    size_max=18,
                    symbol='Gender',
                    opacity=0.7,
                    title='3D Scatter Plot of Loan Data by Loan Status and Gender',
                    labels={
                        'LoanAmount': 'Loan Amount',
                        'ApplicantIncome': 'Applicant Income',
                        'LoanAmountTerm': 'Loan Term',
                        'LoanStatus': 'Loan Status',
                        'CreditHistory': 'Credit History',
                        'Gender': 'Gender'
                    })

fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),
                  scene=dict(
                      xaxis_title='Loan Amount',
                      yaxis_title='Applicant Income',
                      zaxis_title='Loan Term',
                  ))

fig.show()

####################################################
dfGroupBy = pd.DataFrame(df.groupby(['Gender'])['Married'].count())
dfGroupBy.reset_index(inplace=True)

#fig = px.sunburst(dfGroupBy, path=['Gender'], values='Married')
#fig.update_layout(title="Placement % of Married by gender", title_x=0.5)
#fig.show()


####################################################
display_colored_message("PLOT DISTRIBUTION OF SELFEMPLOYED...")
def single_plot_distribution(column_name, dataframe, title):

    # Get the value counts of the specified column
    value_counts = dataframe[column_name].value_counts()

    # Set up the figure with two subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6), facecolor='black')

    # Set main title for the figure
    fig.suptitle(title)

    # Pie chart
    #pie_colors = ['#0077b6', '#00b4d8', '#90e0ef', '#caf0f8']
    pie_colors = ['#0077b6', '#ff4500', '#90e0ef', '#caf0f8']
    ax1.pie(value_counts, autopct='%0.001f%%', startangle=100, textprops={'fontsize': 20}, pctdistance=0.75, colors=pie_colors, labels=None)
    centre_circle = plt.Circle((0,0),0.40,fc='black')
    ax1.add_artist(centre_circle)
    ax1.set_title(f'Distribution of {column_name}', fontsize=16, color='white')

    # Bar chart
    sns.barplot(x=value_counts.index, y=value_counts.values, ax=ax2, palette=pie_colors)
    ax2.set_title(f'Count of {column_name}', fontsize=16, color='white')
    ax2.set_xlabel(column_name, fontsize=14, color='white')
    ax2.set_ylabel('Count', fontsize=14, color='white')

    # Rotate x-axis labels for better readability
    ax2.tick_params(axis='x', rotation=45, colors='white')
    ax2.tick_params(axis='y', colors='white')

    # Set background color for the subplots
    ax1.set_facecolor('black')
    ax2.set_facecolor('black')

    # Show the plots
    plt.tight_layout()
    plt.show()


# Plot the distribution of SelfEmployed in the dataset
single_plot_distribution('SelfEmployed', df, 'SelfEmployed Distribution')


####################################################
display_colored_message("\nGROUP BY GENDER AND CALCULATE MEAN LOAN AMOUNT...")
print("\n")
# Group the DataFrame by 'Gender' and calculate the mean of 'LoanAmount'
np = round(df.groupby('Gender')['LoanAmount'].mean().round(1))

def bar_charts(x, y, title):
    # Create a bar chart using Plotly Express
    fig = px.bar(
        x=x,  # Data for the x-axis
        y=y,  # Data for the y-axis
        title=title,  # Title of the chart
        color=y,  # Color the bars based on y-values
        labels={'x': 'Gender', 'y': 'Average LoanAmount'},  # Custom axis labels
        text=y  # Add text labels to the bars
        ,width=900, height=500
    )

    # Use a dark template
    fig.update_layout(template='plotly_dark')

    fig.show()

# Call the bar_charts function to generate a bar chart
bar_charts(
    np.index,           # Data for the x-axis (gender)
    np.values,          # Data for the y-axis (average loan Amount)
    'Average Loan Amount by Gender'  # Title of the chart
)

####################################################
display_colored_message("\nMEAN APPLICANT INCOME BY LOAN STATUS...")

# Group by 'Loan_Status' and calculate mean 'ApplicantIncome'
income_mean = df.groupby('Loan_Status')['ApplicantIncome'].mean().reset_index()

# Create advanced bar chart using Plotly Express
fig = px.bar(income_mean, x='Loan_Status', y='ApplicantIncome',
             color='Loan_Status',
             labels={'ApplicantIncome': 'Mean Applicant Income'},
             title='Mean Applicant Income by Loan Status',
             text='ApplicantIncome',
             width=900, height=500)

# Update layout
fig.update_layout(xaxis_title='Loan Status', yaxis_title='Mean Applicant Income',
                  showlegend=False, barmode='group')

# Show the plot
fig.show()

####################################################
display_colored_message("\nLOAN AMOUNT DISTRIBUTION...")
fig, axs1 = plt.subplots(1, 3, figsize=(18, 6))

# Create violin plots for ApplicantIncome, CoapplicantIncome, and LoanAmount
sns.violinplot(data=df, y="ApplicantIncome", ax=axs1[0], color='green')
axs1[0].set_title('Applicant Income Distribution')

sns.violinplot(data=df, y="CoapplicantIncome", ax=axs1[1], color='skyblue')
axs1[1].set_title('Coapplicant Income Distribution')

sns.violinplot(data=df, y="LoanAmount", ax=axs1[2], color='orange')
axs1[2].set_title('Loan Amount Distribution')

# Adjust layout
plt.tight_layout()
plt.show()

####################################################
display_colored_message("\nLOAN STATUS DISTRIBUTION - LOAN TERM AMOUNT...")

# Set the style and context for seaborn
sns.set_context("paper", font_scale=1.1)

# Create two subplots side by side
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: Distribution of Loan Amount Term by Loan Approval
sns.kdeplot(df.LoanAmountTerm[df["Loan_Status"] == 0], color="Red", shade=True, ax=axes[0])
sns.kdeplot(df.LoanAmountTerm[df["Loan_Status"] == 1], color="Blue", shade=True, ax=axes[0])
axes[0].legend(["No Loan", "Loan"], loc='upper right')
axes[0].set_ylabel('Density')
axes[0].set_xlabel('Loan Amount Term')
axes[0].set_title('Distribution of Loan Amount Term by Loan Approval')

# Plot 2: Distribution of Loan Amount by Loan Approval
sns.kdeplot(df.LoanAmount[df["Loan_Status"] == 0], color="Red", shade=True, ax=axes[1])
sns.kdeplot(df.LoanAmount[df["Loan_Status"] == 1], color="Blue", shade=True, ax=axes[1])
axes[1].legend(["No Loan", "Loan"], loc='upper right')
axes[1].set_ylabel('Density')
axes[1].set_xlabel('Loan Amount')
axes[1].set_title('Distribution of Loan Amount by Loan Approval')

# Adjust layout to prevent overlap
plt.tight_layout()

# Show the plots
plt.show()

####################################################
display_colored_message("\nPIE VISULAZATION...")

#DATA visulazation
d_labels = ['Male', 'Female']
g_labels = ['Graduate', 'Not Graduate']
l_labels = ['YES', 'NO']
p_labels = ['Semiurban', 'Urban', 'Rural']

# Create subplots: use 'domain' type for Pie subplot
fig = make_subplots(rows=1, cols=4, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])
fig.add_trace(go.Pie(labels=d_labels, values=df['Gender'].value_counts(), name="Gender"),
              1, 1)
fig.add_trace(go.Pie(labels=g_labels, values=df['Education'].value_counts(), name="Education"),
              1, 2)
fig.add_trace(go.Pie(labels=p_labels, values=df['PropertyArea'].value_counts(), name="PropertyArea"),
              1, 3)
fig.add_trace(go.Pie(labels=l_labels, values=df['Loan_Status'].value_counts(), name="Loan_Status"),
              1, 4)


# Use `hole` to create a donut-like pie chart
fig.update_traces(hole=.4, hoverinfo="label+percent+name", textfont_size=16)

fig.update_layout(
    title_text="Gender Education and Loan Status Distributions",
    # Add annotations in the center of the donut pies.
    annotations=[dict(text='Gender', x=0.1, y=0.3, font_size=20, showarrow=False),
                 dict(text='Education', x=0.3, y=0.6, font_size=20, showarrow=False),
                 dict(text='PropertyArea', x=0.6, y=0.9, font_size=20, showarrow=False),
                 dict(text='Loan Status', x=0.9, y=0.5, font_size=20, showarrow=False)])
fig.show()

####################################################
display_colored_message("\nPIE LOAN STATUS DISTRIBUTION VS DEPENDENTS...")

# Find the most repeated Dependents value for each Loan_Status
mode_dependents = df.groupby('Loan_Status')['Dependents'].apply(lambda x: x.mode().iloc[0]).to_dict()

# Replace None values in Dependents column based on Loan_Status
for index, row in df.iterrows():
    if pd.isnull(row['Dependents']):
        df.at[index, 'Dependents'] = mode_dependents[row['Loan_Status']]

# Group by dependents the employees who got a loan
loan_yes_counts = df[df["Loan_Status"] == 1].groupby("Dependents").size().tolist()

# Group by dependents the employees who didn't get a loan
loan_no_counts = df[df["Loan_Status"] == 0].groupby("Dependents").size().tolist()


# Group by dependents the employees who got a loan
loan_yes_counts = df[df["Loan_Status"] == 1].groupby("Dependents").size()

# Group by dependents the employees who didn't get a loan
loan_no_counts = df[df["Loan_Status"] == 0].groupby("Dependents").size()

# Data for the outer pie chart (distribution of loan status by dependents)
labels_dependents = ["Zero", "One", "Two", "Three+", "Zero", "One", "Two", "Three+"]
sizes_dependents = [
    loan_yes_counts.get('0', 0), loan_yes_counts.get('1', 0), loan_yes_counts.get('2', 0), loan_yes_counts.get('3+', 0),
    loan_no_counts.get('0', 0), loan_no_counts.get('1', 0), loan_no_counts.get('2', 0), loan_no_counts.get('3+', 0)
]

labels_loan =["Loan: Yes", "Loan: No"]
values_loan = [sum(loan_yes_counts), sum(loan_no_counts)]

# Colors
colors_loan = ['#ff6666', '#66b3ff']
colors_dependents = ['#d8d8e9', '#c2c2f0', '#ffb3e6', '#b3ffb3', '#d8d8e9', '#c2c2f0', '#ffb3e6', '#b3ffb3']

# Explode
explode = (0.3,0.3)
explode_dependents = (0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)

# Plot
plt.figure(figsize=(10, 10))
textprops = {"fontsize":10}

#Plot
plt.pie(values_loan, labels=labels_loan,autopct='%1.1f%%',pctdistance=1.08, labeldistance=0.8,colors=colors_loan, startangle=90,frame=True, explode=explode,radius=10, textprops =textprops, counterclock = True, )
plt.pie(sizes_dependents,labels=labels_dependents,colors=colors_dependents,startangle=90, explode=explode_dependents,radius=7, textprops =textprops, counterclock = True, )
#Draw circle
centre_circle = plt.Circle((0,0),5,color='black', fc='white',linewidth=0)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

# Set equal aspect ratio
plt.axis('equal')

plt.title('Loan Status Distribution vs Dependents: Zero, One, Two, Three+', fontsize=14, y=1)
plt.show()


##################################################
display_colored_message("\n LOAN STATUS DISTRIBUTION...")

# Create a figure with multiple subplots
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Plot 1: Property Area vs. Loan Status
sns.countplot(x='PropertyArea', hue='Loan_Status', data=df, ax=axes[0, 0])
axes[0, 0].set_title('Property Area vs. Loan Status')

# Plot 2: Credit History vs. Loan Status
sns.countplot(x='CreditHistory', hue='Loan_Status', data=df, ax=axes[0, 1])
axes[0, 1].set_title('Credit History vs. Loan Status')

# Plot 3: Gender vs. Loan Status
sns.countplot(x='Gender', hue='Loan_Status', data=df, ax=axes[0, 2])
axes[0, 2].set_title('Gender vs. Loan Status')

# Plot 4: Loan Amount Term vs. Loan Status
sns.countplot(x='LoanAmountTerm', hue='Loan_Status', data=df, ax=axes[1, 0])
axes[1, 0].set_title('Loan Amount Term vs. Loan Status')

# Plot 5: Self Employed vs. Loan Status
sns.countplot(x='SelfEmployed', hue='Loan_Status', data=df, ax=axes[1, 1])
axes[1, 1].set_title('Self Employed vs. Loan Status')

# Plot 6: Married vs. Loan Status
sns.countplot(x='Married', hue='Loan_Status', data=df, ax=axes[1, 2])
axes[1, 2].set_title('Married vs. Loan Status')

# Adjust layout to prevent overlap
plt.tight_layout()

# Show the plot
plt.show()

##################################################
display_colored_message("\n DISTRIBUTION BASED ON COUNT...")

num_cols = 3
num_rows = (len(df.columns) + num_cols - 1) // num_cols

# Create a figure and axes
fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))

# Flatten the axes if needed
axes = axes.flatten()

# Loop through each column and plot the distribution
for i, column in enumerate(df.columns):
    sns.histplot(df[column], kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {column} based on count')

# Hide any unused subplots
for j in range(i+1, len(axes)):
    axes[j].axis('off')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

# Create subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))

# Flatten the axes if needed
axes = axes.flatten()

# Iterate through each feature column (excluding the target variable)
for i, column in enumerate(df.columns[:-1]):
    # Plot scatter plot
    sns.scatterplot(x=df[column], y=df['Loan_Status'], ax=axes[i])
    axes[i].set_title(f'Scatter Plot: {column} vs Loan_Status')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Loan_Status')

# Hide any unused subplots
for j in range(i+1, len(axes)):
    axes[j].axis('off')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

##############################################
# Create a pairplot
sns.pairplot(df)

# Display the pairplot
plt.show()

##############################################
display_colored_message("APPLY LABEL ENCODER FOR OBJECT COLUMN TYPE...")
# Filter columns with 'object' dtype
object_columns = df.select_dtypes(include=['object']).columns

# Initialize LabelEncoder
le = LabelEncoder()

# Apply LabelEncoder to 'object' columns
for col in object_columns:
    df[col] = le.fit_transform(df[col])


print(colored(tabulate(df.head(1), headers='keys', tablefmt='fancy_grid'), 'cyan'))

##############################################
display_colored_message("SAVE PROCESSED CSV FILE ...")
#Save the processed data to a new CSV file
df.to_csv('/content/sample_data/loan_processed.csv', index=True)

##############################################

display_colored_message("MODEL DATA SPLIT..")

# Dropping multiple columns in a single call
columns_to_drop = ['SelfEmployed']
#columns_to_drop = ['SelfEmployed', 'Gender', 'Education', 'Dependents', 'Married', 'PropertyArea']

# Extend ALL_DROPPED_COLUMNS with elements of columns_to_drop if they do not already exist
for column in columns_to_drop:
    if column not in ALL_DROPPED_COLUMNS:
        ALL_DROPPED_COLUMNS.append(column)


df_train = df.copy()
df_train = df_train.drop(columns=columns_to_drop)

print (df_train.shape)
X = df_train
y = df_train.Loan_Status

X = X.drop('Loan_Status',axis=1) #Remove label


#SMOTE for imbalenced Dat
#smote=SMOTE()
#X,y=smote.fit_resample(X,y)

# Rescale and normalize the features
# Standardization (Normalization)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Min-Max Scaling (Rescaling)
#scaler = MinMaxScaler()
#X_scaled = scaler.fit_transform(X)

# Data Splitting

X_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size = 0.2,
random_state = 42)


print(colored(tabulate(X.head(2), headers='keys', tablefmt='fancy_grid'), 'cyan'))

display_colored_sub_message("\nAFTER Scaler...")

# Convert X_train to a DataFrame
print(colored(tabulate(pd.DataFrame(X_scaled).head(2), headers='keys', tablefmt='fancy_grid'), 'cyan'))

##############################################
display_colored_message("MODELS SCORE AND CONFUSION MATRIX...")

# List of models to train
models = [
    ("LogisticRegression", LogisticRegression(max_iter=200)),
    ("DecisionTree", DecisionTreeClassifier()),
    ("RandomForest", RandomForestClassifier(n_estimators=150,max_depth=17,
                          min_samples_leaf=10,min_samples_split=9)),
    ("KNearestNeighbors", KNeighborsClassifier(n_neighbors=5)),
    ("NaiveBayesGNB", GaussianNB()),
    ("NaiveBayesBNB", BernoulliNB()),
    ("SVM", SGDClassifier(loss='log', random_state=42)),
    ("MLPClassifier", MLPClassifier()),
    ("XGBoost", XGBClassifier())
]

# Create a DataFrame to store the evaluation metrics for each model
model_metrics = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score'])

# Function to calculate evaluation metrics and add them to the DataFrame
def evaluate_model(model, model_name, X_test, y_test):
    # Predict labels
    y_pred = model.predict(X_test)

    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Add metrics to the DataFrame
    model_metrics.loc[len(model_metrics)] = [model_name, accuracy, precision, recall, f1]

# Train and evaluate models
fig, axes = plt.subplots(3, 3, figsize=(18, 15))
axes = axes.ravel()

# Train each model and plot confusion matrix
for i, (name, model) in enumerate(models):
    model.fit(X_train, y_train)
    predicted_y = model.predict(X_test)

    scoreTrain = model.score(X_train, y_train)
    scoreTest = model.score(X_test, y_test)

    # Print model header with color
    print(colored(f"\n{'='*40}\n{name} Model\n{'='*40}", 'blue'))
    print(colored(f"Train Accuracy: {scoreTrain:.3f}", 'green'))
    print(colored(f"Test Accuracy: {scoreTest:.3f}\n", 'green'))

    # Print classification report with color
    print(colored("Classification Report:", 'blue'))
    report = classification_report(y_test, predicted_y)
    print(colored(report, 'yellow'))

    sns.heatmap(confusion_matrix(y_test, predicted_y), annot=True, fmt="d", linecolor="k", linewidths=3, ax=axes[i])
    axes[i].set_title(f"{name} CONFUSION MATRIX", fontsize=14)

    evaluate_model(model, name, X_test, y_test)


plt.tight_layout()
plt.show()

display_colored_message("MODELS COMPARISON...")

##############################################
display_colored_message("ROC CURVE AND AUC SCORE...")
# ROC Curve and AUC Score

# Define dictionaries to store ROC AUC scores and curves
roc_auc_scores = {}
fpr_tpr = {}

# Calculate ROC AUC scores and curves for each model
for name, model in models:
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    fpr, tpr, p = roc_curve(y_test, y_pred_proba)
    roc_auc_scores[name] = roc_auc
    fpr_tpr[name] = (fpr, tpr)

# Plot ROC curves
plt.figure()
for name in roc_auc_scores:
    fpr, tpr = fpr_tpr[name]
    plt.plot(fpr, tpr, label=f'{name} (area = {roc_auc_scores[name]:.2f})')

plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

##############################################
display_colored_message("MODEL METRICS...")
# Apply styles to highlight the highest metric for each model
def highlight_max(s):
    is_max = s == s.max()
    return ['background-color: yellow' if v else '' for v in is_max]

# Apply the style to the DataFrame
styled_model_metrics = model_metrics.style.apply(highlight_max, subset=['Accuracy', 'Precision', 'Recall', 'F1-score'])

# Save the styled DataFrame to an HTML file
styled_model_metrics_file = "model_metrics.html"
styled_model_metrics_html = styled_model_metrics.to_html()
with open(styled_model_metrics_file, "w") as f:
    f.write(styled_model_metrics_html)

print("Styled model metrics saved to:", styled_model_metrics_file)
# Display the HTML output
display(HTML(styled_model_metrics_html))


##############################################
display_colored_message("EVALUATION METRICS...")

# Define the models and their corresponding evaluation metrics
models_list = [name for name, model in models]
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']

# Initialize lists to store the metrics for each model
accuracies = []
precisions = []
recalls = []
f1_scores = []

# Populate the lists with the metrics for each model
for model_name in models_list:
    model_metrics_row = model_metrics[model_metrics['Model'] == model_name].iloc[0]
    accuracies.append(model_metrics_row['Accuracy'])
    precisions.append(model_metrics_row['Precision'])
    recalls.append(model_metrics_row['Recall'])
    f1_scores.append(model_metrics_row['F1-score'])

#####################
# Create a DataFrame from the metrics lists
metrics_df = pd.DataFrame({
    'Model': models_list,
    'Accuracy': accuracies,
    'Precision': precisions,
    'Recall': recalls,
    'F1-score': f1_scores
})

# 3D scatter plot comparison between models
fig = px.scatter_3d(
    metrics_df,
    x='Accuracy',
    y='Precision',
    z='Recall',
    color='F1-score',  # Use F1-score for color to represent overall performance
    size='F1-score',   # Use F1-score for size to emphasize models with higher F1-scores
    size_max=18,
    symbol='Model',    # Use Model for symbols to differentiate between different models
    opacity=0.7
)


fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))

# Display the plot
fig.show()



# 3D scatter plot comparison between models
fig = px.scatter_3d(
    metrics_df,
    x='Accuracy',
    y='F1-score',
    z='Precision',  # Use Precision for z-axis
    color='Model',  # Use Model for color to differentiate between different models
    size='F1-score',   # Use F1-score for size to emphasize models with higher F1-scores
    size_max=18,
    symbol='Model',    # Use Model for symbols to differentiate between different models
    opacity=0.7
)

# Update layout for better visualization
fig.update_layout(
    title='Model Performance Comparison',
    scene=dict(
        xaxis_title='Accuracy',
        yaxis_title='F1-score',
        zaxis_title='Precision'
    )
)

# Display the figure
fig.show()


##############################################
display_colored_message("MODEL PERFORMANCE BY METRICS...")

# Bar plot comparison between metrics for each model
fig = go.Figure()
for model_name in models_list:
    model_index = models_list.index(model_name)
    metrics_values = [accuracies[model_index], precisions[model_index], recalls[model_index], f1_scores[model_index]]
    fig.add_trace(go.Bar(
        x=metrics,
        y=metrics_values,
        name=model_name,
        marker=dict(color=px.colors.qualitative.Plotly[model_index]),
        width=0.08
    ))

fig.update_layout(
    barmode='group',
    title='Model Performance Comparison by Metric',
    xaxis_title='Metrics',
    yaxis_title='Score',
    legend_title='Model',
    width=1400,  # Adjust the width of the plot
    height=500  # Optionally adjust the height of the plot
)
fig.show()

##############################################
display_colored_message("MODEL RADAR CHART COMPARAISON...")
# Radar chart comparison between metrics for each model
fig = go.Figure()
for model_name in models_list:
    model_index = models_list.index(model_name)
    fig.add_trace(go.Scatterpolar(
        r=[accuracies[model_index], precisions[model_index], recalls[model_index], f1_scores[model_index]],
        theta=metrics,
        fill='toself',
        name=model_name,
        marker=dict(color=px.colors.qualitative.Plotly[model_index]),
    ))

fig.update_layout(
    polar=dict(
        radialaxis=dict(
            visible=True,
            range=[0, 1]
        )),
    showlegend=True,
    title='Model Performance Comparison by Metric',
    width=1200,
    height=800,
)
fig.show()

##############################################
display_colored_message("MODEL PERFORMANCE PARALLEL COORDINATES...")
# Parallel coordinates plot for model performance
scores = {'Model': models_list}
for metric in metrics:
    scores[metric] = [model_metrics[model_metrics['Model'] == model_name].iloc[0][metric] for model_name in models_list]

parallel_data = pd.DataFrame(scores)
colors = plt.cm.tab10(range(len(models_list)))


plt.figure(figsize=(12, 6))
parallel_coordinates(parallel_data, 'Model', colormap='viridis', color=colors)
plt.legend(loc='upper right')
plt.xlabel('Metrics')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.title('Parallel Coordinates Plot of Model Performance')
plt.show()


##############################################
display_colored_message("MODEL TEST...")

# Given DataFrame
testingData = pd.DataFrame({
    'Loan_ID': ['LP001015', 'LP001022', 'LP001031'],
    'Gender': ['Male', 'Male', 'Male'],
    'Married': ['Yes', 'Yes', 'Yes'],
    'Dependents': ['0', '1', '2'],
    'Education': ['Graduate', 'Graduate', 'Graduate'],
    'SelfEmployed': ['No', 'No', 'No'],
    'ApplicantIncome': [5720, 3076, 5000],
    'CoapplicantIncome': [0, 1500, 1800],
    'LoanAmount': [110, 126, 208],
    'LoanAmountTerm': [360, 360, 360],
    'CreditHistory': [1, 1, 1],
    'PropertyArea': ['Urban', 'Urban', 'Urban'],
})


path = '/content/sample_data/'
testingData = pd.read_csv(path + 'loan_test.csv')   

testDf = testingData.copy()

display_colored_message("RENAME COLUMNS...")

#RENAME COLUMNS
testDf = rename_columns(testDf)
print(testDf.columns)

display_colored_sub_message("ORIGINAL TESTING DATASET...")

print(colored(tabulate(testDf.head(2), headers='keys', tablefmt='fancy_grid'), 'cyan'))

#REMOVE SAME COLUMNS
print(ALL_DROPPED_COLUMNS)

#FILL MISSING VALUES
data_prep_fill_mising_values(testDf)

#DROP SAME COLUMNS
testDf.drop(ALL_DROPPED_COLUMNS, axis=1, inplace=True)

display_colored_sub_message("TESTING DATASET NO MISSING  & DROP COLUMNS...")
print(colored(tabulate(testDf.head(2), headers='keys', tablefmt='fancy_grid'), 'cyan'))

le = LabelEncoder()

# Apply LabelEncoder to 'object' columns
object_columns = testDf.select_dtypes(include=['object']).columns
for col in object_columns:
    testDf[col] = le.fit_transform(testDf[col])

#df_train = df_train.drop(columns=['Loan_Status'])

display_colored_sub_message("TESTING DATASET LAPPLY LABEL ENCODER...")
print(colored(tabulate(testDf.head(2), headers='keys', tablefmt='fancy_grid'), 'cyan'))

# Fit and transform the data
scaled_team_data = scaler.transform(testDf)

display_colored_sub_message("TESTING DATASET APPLY SCALER...")
# Convert X_train to a DataFrame
print(colored(tabulate(pd.DataFrame(scaled_team_data).head(2), headers='keys', tablefmt='fancy_grid'), 'cyan'))

# Assuming you have trained your models list previously

#"LogisticRegression" "DecisionTree" "RandomForest" "K NearestNeighbors" "NaiveBayesGNB"
#"SVM" "NaiveBayesBNB" "MLPClassifier" "XGBoost"

#MODEL_SELECTED = "NaiveBayesBNB"
MODEL_SELECTED = "SVM"

display_colored_sub_message(f"TESTING DATASET APPLY MODEL : {MODEL_SELECTED}...")

# Find Best model in the models list
best_model = None
for name, model in models:
    if name == MODEL_SELECTED:
        best_model = model
        break

# Check if best_model model is found
if best_model is not None:
    # Assuming you have your data for prediction stored in X_pred
    # Make predictions using best_model model
    predictions = best_model.predict(scaled_team_data)
    # Now you have predictions using best_model model
    #print(f"{MODEL_SELECTED} prediction:", predictions)

    testingData['Loan_Status'] = predictions
else:
    print(f"{MODEL_SELECTED} model not found in the models list.")

display_colored_message(f"TESTING DATASET WITH PREDICTION...")
print(colored(tabulate(testingData.head(2), headers='keys', tablefmt='fancy_grid'), 'cyan'))

testOutputFileName = path + 'loan_test_output.csv'
display_colored_sub_message(f"GENERATE THE NEW DATASET WITH PREDICTION {testOutputFileName}")
# Assuming df is your DataFrame
testingData.to_csv(testOutputFileName, index=False)

display_colored_message("DATA VISULAZATION...")

# Counting the accurance of each value in Gender column
testingData['Gender'].value_counts()

# comparing loan status with gender column
plt.figure(figsize=(8,3))
sns.countplot(x = 'Gender',hue ='Loan_Status', data=testingData , palette='hls')
plt.show()


####################################################
display_colored_message("MODEL IMPORTANCE FEATURES...")
#Model importance feature

#feature_labels = np.array(['Gender', 'Married', 'Dependents', 'Education', 'SelfEmployed',
#                           'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',
#         'LoanAmountTerm', 'PropertyArea', 'CreditHistory'])

# Get feature labels from DataFrame columns (excluding the last column)
feature_labels = df_train.columns[:-1].values

# Function to display feature importance
def display_feature_importance(model, model_name, feature_labels):
    if hasattr(model, "feature_importances_"):
        importance = model.feature_importances_
    elif hasattr(model, "coef_"):
        importance = model.coef_[0]
    else:
        importance = []

    print(f"\033[1m\033[34m{'='*40}\nFeature importance for {model_name}:\n{'='*40}\033[0m")
    # Creating a series for feature importances
    feat_importances = pd.Series(importance, index=feature_labels)
    feat_importances.nlargest(12).plot(kind='barh')
    plt.title(f"Feature Importance for {model_name}")
    plt.show()

    feature_indexes_by_importance = importance.argsort()
    print(f"\033[1m\033[34m{'='*40}\nFeature importance for {model_name}:\n{'='*40}\033[0m")
    for index in feature_indexes_by_importance:
        importance_percentage = importance[index] * 100.0
        feature_name = feature_labels[index]
        color_code = '\033[32m' if importance_percentage > 10 else '\033[33m' if importance_percentage > 5 else '\033[0m'
        print(f"{color_code}{feature_name}: {importance_percentage:.2f}%\033[0m")
    print()


# Display feature importance for applicable models
for i in range(len(models)):
    name, model_instance = models[i]
    display_feature_importance(model_instance, name, feature_labels)




#############################################################
display_colored_message("DEEP LEARNING...")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

print("Deep learning");

# Build the model
model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy*100:.2f}%')



# Calculate class weights to handle imbalanced data
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))


# Enhanced deep learning model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

optimizer = Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Training the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2,
                    class_weight=class_weights_dict, callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
y_pred = model.predict(X_test).flatten()
roc_auc = roc_auc_score(y_test, y_pred)

print(f'Deep Learning Model Accuracy: {accuracy * 100:.2f}%')
print(f'Deep Learning Model AUC-ROC: {roc_auc:.2f}')



from kerastuner.tuners import RandomSearch

def build_model(hp):
    model = Sequential()
    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=32), activation='relu', input_dim=X_train.shape[1]))
    model.add(Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1)))
    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=512, step=32), activation='relu'))
    model.add(Dropout(hp.Float('dropout_2', 0.2, 0.5, step=0.1)))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='log')),
                  loss='binary_crossentropy', metrics=['accuracy'])
    return model

tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=10,
    executions_per_trial=3,
    directory='my_dir',
    project_name='loan_approval'
)

tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])

# Get the optimal hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"Best hyperparameters: {best_hps.values}")

# Train the best model
best_model = tuner.hypermodel.build(best_hps)
history = best_model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])

# Evaluate the best model
loss, accuracy = best_model.evaluate(X_test, y_test)
y_pred = best_model.predict(X_test).flatten()
roc_auc = roc_auc_score(y_test, y_pred)

print(f'Best Deep Learning Model Accuracy: {accuracy * 100:.2f}%')
print(f'Best Deep Learning Model AUC-ROC: {roc_auc:.2f}')

